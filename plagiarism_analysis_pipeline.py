#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Comprehensive Textbook Plagiarism Analysis Pipeline
–ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –∞–Ω–∞–ª–∏–∑–∞ —É—á–µ–±–Ω–∏–∫–æ–≤ –Ω–∞ –ø–ª–∞–≥–∏–∞—Ç

Features:
- Multi-level caching (OCR text, extracted entities, analysis results)
- Human-readable cache files
- Advanced entity extraction
- Modular pipeline architecture
- Progress tracking
"""

import os
import re
import json
import hashlib
import pandas as pd
import numpy as np
from datetime import datetime
from collections import Counter, defaultdict
from difflib import SequenceMatcher
from pathlib import Path

# PDF and OCR libraries
import PyPDF2
import pdfplumber
import fitz  # PyMuPDF
import pytesseract
from pdf2image import convert_from_path
from PIL import Image
import cv2

# Text analysis - using only built-in difflib for simplicity

class CachedTextbookAnalyzer:
    def __init__(self, source_pdf_path, suspect_pdf_path, cache_dir="cache"):
        """
        Initialize the comprehensive analyzer with caching
        
        Args:
            source_pdf_path: Path to source textbook
            suspect_pdf_path: Path to suspected plagiarism textbook  
            cache_dir: Directory for cache files
        """
        self.source_path = source_pdf_path
        self.suspect_path = suspect_pdf_path
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
        
        # Create subdirectories for different cache types
        (self.cache_dir / "ocr_text").mkdir(exist_ok=True)
        (self.cache_dir / "entities").mkdir(exist_ok=True)
        (self.cache_dir / "analysis").mkdir(exist_ok=True)
        
        # Configuration
        self.config = {
            'ocr': {
                'language': 'rus+eng',
                'dpi': 200,
                'max_pages': 10000,  # Process all pages
                'batch_size': 25,    # Smaller batches for stability
                'min_text_length': 5, # More permissive minimum text length
                'min_confidence': 20  # Lower confidence threshold
            },
            'similarity': {
                'threshold': 0.15,  # Further lowered to catch more potential borrowings
                'min_phrase_length': 20,  # Minimum characters for matching phrase
                'min_words': 3,  # Minimum words for matching phrase
                'paraphrase_threshold': 0.8  # Similarity threshold for paraphrases
            },
            'entities': {
                'question_confidence_threshold': 0.8,
                'min_question_length': 10,
                'max_question_length': 500
            }
        }
        
        self.results = {}
        
    def _get_file_hash(self, file_path):
        """Generate hash for file to detect changes"""
        with open(file_path, 'rb') as f:
            # Read first and last 1MB for hash (efficient for large PDFs)
            first_chunk = f.read(1024 * 1024)
            f.seek(-1024 * 1024, 2)
            last_chunk = f.read(1024 * 1024)
            
        return hashlib.md5(first_chunk + last_chunk).hexdigest()
    
    def _get_cache_path(self, cache_type, file_path, suffix=""):
        """Get cache file path for given type and file"""
        file_hash = self._get_file_hash(file_path)
        filename = f"{Path(file_path).stem}_{file_hash}{suffix}.json"
        return self.cache_dir / cache_type / filename
    
    def _save_human_readable_cache(self, data, cache_path, title="Cache Data"):
        """Save cache in human-readable JSON format"""
        cache_data = {
            'metadata': {
                'title': title,
                'created_at': datetime.now().isoformat(),
                'file_path': str(data.get('source_file', 'unknown')),
                'total_pages': data.get('total_pages', 0),
                'extraction_method': data.get('extraction_method', [])
            },
            'data': data
        }
        
        with open(cache_path, 'w', encoding='utf-8') as f:
            json.dump(cache_data, f, ensure_ascii=False, indent=2)
    
    def _load_cache(self, cache_path):
        """Load cache data if exists and valid"""
        if cache_path.exists():
            try:
                with open(cache_path, 'r', encoding='utf-8') as f:
                    cache_data = json.load(f)
                print(f"üìÅ Loaded cache: {cache_path.name}")
                return cache_data.get('data', cache_data)
            except Exception as e:
                print(f"‚ö†Ô∏è Cache file corrupted, will regenerate: {e}")
        return None
    
    def extract_text_with_cache(self, pdf_path):
        """Extract text with caching support"""
        cache_path = self._get_cache_path("ocr_text", pdf_path, "_text")
        cached_data = self._load_cache(cache_path)
        
        if cached_data:
            return cached_data
        
        print(f"üîÑ Extracting text from: {os.path.basename(pdf_path)}")
        
        # Initialize content structure
        content = {
            'source_file': pdf_path,
            'pages_with_text': 0,
            'total_pages': 0,
            'text_pages': [],  # List of {page_number, text, method, char_count, confidence}
            'images': [],
            'extraction_method': [],
            'extraction_stats': {
                'normal_extraction_pages': 0,
                'ocr_extraction_pages': 0,
                'failed_pages': 0,
                'total_characters': 0
            }
        }
        
        try:
            # Get total pages
            doc = fitz.open(pdf_path)
            content['total_pages'] = len(doc)
            
            # Try normal text extraction first
            normal_text_pages = 0
            for page_num in range(min(10, len(doc))):  # Sample first 10 pages
                page = doc.load_page(page_num)
                text = page.get_text().strip()
                if len(text) > self.config['ocr']['min_text_length']:
                    normal_text_pages += 1
            
            # Decide extraction method
            use_ocr = normal_text_pages < min(3, len(doc) * 0.2)  # If <20% pages have text
            
            if use_ocr:
                print(f"üîç Using OCR (normal extraction gave {normal_text_pages} pages)")
                content = self._extract_with_advanced_ocr(pdf_path, content, doc)
            else:
                print(f"üìÑ Using normal text extraction")
                content = self._extract_with_pymupdf(pdf_path, content, doc)
            
            doc.close()
            
        except Exception as e:
            print(f"‚ùå Error extracting from {pdf_path}: {e}")
            content['extraction_method'].append(f'Error: {e}')
        
        # Save to cache
        self._save_human_readable_cache(
            content, cache_path, 
            f"Text Extraction - {os.path.basename(pdf_path)}"
        )
        
        return content
    
    def _extract_with_pymupdf(self, pdf_path, content, doc):
        """Normal text extraction using PyMuPDF"""
        for page_num in range(len(doc)):
            page = doc.load_page(page_num)
            text = page.get_text().strip()
            
            if len(text) > self.config['ocr']['min_text_length']:
                content['pages_with_text'] += 1
                content['text_pages'].append({
                    'page_number': page_num + 1,
                    'text': text,
                    'method': 'PyMuPDF',
                    'char_count': len(text),
                    'confidence': 1.0  # High confidence for direct extraction
                })
                content['extraction_stats']['normal_extraction_pages'] += 1
                content['extraction_stats']['total_characters'] += len(text)
            
            # Count images
            image_list = page.get_images()
            for img_index, img in enumerate(image_list):
                content['images'].append({
                    'page': page_num + 1,
                    'img_index': img_index,
                    'xref': img[0]
                })
        
        content['extraction_method'].append('PyMuPDF')
        return content
    
    def _extract_with_advanced_ocr(self, pdf_path, content, doc):
        """Advanced OCR extraction with confidence scoring"""
        max_pages = min(self.config['ocr']['max_pages'], content['total_pages'])
        batch_size = self.config['ocr']['batch_size']
        
        print(f"üìñ OCR processing {max_pages} pages in batches of {batch_size}")
        print(f"üìã OCR settings: min_text_length={self.config['ocr']['min_text_length']}, min_confidence={self.config['ocr']['min_confidence']}")
        
        total_processed = 0
        successful_extractions = 0
        for start_page in range(0, max_pages, batch_size):
            end_page = min(start_page + batch_size, max_pages)
            print(f"  Processing pages {start_page + 1}-{end_page}...")
            
            try:
                # Convert PDF pages to images
                images = convert_from_path(
                    pdf_path,
                    first_page=start_page + 1,
                    last_page=end_page,
                    dpi=self.config['ocr']['dpi']
                )
                
                for i, image in enumerate(images):
                    page_num = start_page + i + 1
                    total_processed += 1
                    
                    # Enhanced OCR with confidence
                    ocr_result = self._advanced_ocr_page(image, page_num)
                    
                    if ocr_result['text'] and len(ocr_result['text']) > self.config['ocr']['min_text_length']:
                        successful_extractions += 1
                        content['pages_with_text'] += 1
                        content['text_pages'].append({
                            'page_number': page_num,
                            'text': ocr_result['text'],
                            'method': 'OCR',
                            'char_count': len(ocr_result['text']),
                            'confidence': ocr_result['confidence']
                        })
                        content['extraction_stats']['ocr_extraction_pages'] += 1
                        content['extraction_stats']['total_characters'] += len(ocr_result['text'])
                        print(f"    Page {page_num}: ‚úì ({ocr_result['char_count']} chars, conf: {ocr_result['confidence']:.2f})")
                    else:
                        content['extraction_stats']['failed_pages'] += 1
                        print(f"    Page {page_num}: ‚úó (text_len: {len(ocr_result['text']) if ocr_result['text'] else 0}, conf: {ocr_result['confidence']:.2f})")
                        
                    # Progress update every 20 pages
                    if total_processed % 20 == 0:
                        success_rate = (successful_extractions / total_processed) * 100
                        print(f"    Progress: {total_processed}/{max_pages} pages, {success_rate:.1f}% success rate")
                        
            except Exception as e:
                print(f"    Error in batch {start_page + 1}-{end_page}: {e}")
                content['extraction_stats']['failed_pages'] += (end_page - start_page)
        
        # Final OCR summary
        final_success_rate = (successful_extractions / total_processed) * 100 if total_processed > 0 else 0
        print(f"üìä OCR Summary: {successful_extractions}/{total_processed} pages successfully processed ({final_success_rate:.1f}%)")
        
        # Count images from original PDF
        for page_num in range(len(doc)):
            page = doc.load_page(page_num)
            image_list = page.get_images()
            for img_index, img in enumerate(image_list):
                content['images'].append({
                    'page': page_num + 1,
                    'img_index': img_index,
                    'xref': img[0]
                })
        
        content['extraction_method'].append('Advanced OCR')
        return content
    
    def _advanced_ocr_page(self, image, page_num):
        """Advanced OCR processing for a single page"""
        try:
            # Convert PIL to OpenCV
            opencv_image = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)
            
            # Image preprocessing for better OCR
            gray = cv2.cvtColor(opencv_image, cv2.COLOR_BGR2GRAY)
            
            # Adaptive histogram equalization
            clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
            enhanced = clahe.apply(gray)
            
            # Noise reduction
            denoised = cv2.medianBlur(enhanced, 3)
            
            # OCR with confidence data
            ocr_config = f"--oem 3 --psm 6 -l {self.config['ocr']['language']}"
            
            # Get detailed OCR data
            ocr_data = pytesseract.image_to_data(
                denoised, 
                config=ocr_config,
                output_type=pytesseract.Output.DICT
            )
            
            # Extract text and calculate confidence
            text_parts = []
            confidences = []
            
            for i in range(len(ocr_data['text'])):
                if int(ocr_data['conf'][i]) > self.config['ocr']['min_confidence']:  # Use configurable threshold
                    word = ocr_data['text'][i].strip()
                    if word:
                        text_parts.append(word)
                        confidences.append(int(ocr_data['conf'][i]))
            
            text = ' '.join(text_parts)
            avg_confidence = np.mean(confidences) / 100.0 if confidences else 0.0
            
            return {
                'text': text,
                'confidence': avg_confidence,
                'char_count': len(text),
                'word_count': len(text_parts)
            }
            
        except Exception as e:
            print(f"    OCR error on page {page_num}: {e}")
            return {'text': '', 'confidence': 0.0, 'char_count': 0, 'word_count': 0}
    
    def extract_entities_with_cache(self, text_content):
        """Extract educational entities with caching"""
        # Create cache key based on text content
        content_hash = hashlib.md5(str(text_content).encode()).hexdigest()
        cache_path = self.cache_dir / "entities" / f"entities_{content_hash}.json"
        
        cached_entities = self._load_cache(cache_path)
        if cached_entities:
            return cached_entities
        
        print(f"üîÑ Extracting educational entities...")
        
        entities = {
            'questions': [],
            'assignments': [],
            'film_references': [],
            'internet_sources': [],
            'maps_and_diagrams': [],
            'historical_figures': [],
            'dates_and_periods': [],
            'extraction_stats': {
                'total_text_analyzed': 0,
                'pages_processed': 0,
                'extraction_confidence': {}
            }
        }
        
        # Process each page
        for page_data in text_content.get('text_pages', []):
            text = page_data['text']
            page_num = page_data['page_number']
            
            entities['extraction_stats']['total_text_analyzed'] += len(text)
            entities['extraction_stats']['pages_processed'] += 1
            
            # Extract different entity types
            self._extract_questions_and_assignments(text, page_num, entities)
            self._extract_media_references(text, page_num, entities)
            self._extract_internet_sources(text, page_num, entities)
            self._extract_maps_and_diagrams(text, page_num, entities)
            self._extract_historical_entities(text, page_num, entities)
        
        # Calculate extraction confidence
        total_entities = sum(len(entities[key]) for key in entities if isinstance(entities[key], list))
        entities['extraction_stats']['total_entities'] = total_entities
        entities['extraction_stats']['entities_per_page'] = total_entities / max(1, entities['extraction_stats']['pages_processed'])
        
        # Save to cache
        self._save_human_readable_cache(
            entities, cache_path,
            "Educational Entities Extraction"
        )
        
        return entities
    
    def _extract_questions_and_assignments(self, text, page_num, entities):
        """Advanced question and assignment extraction"""
        
        # Comprehensive question patterns with confidence scoring
        question_patterns = [
            # Direct question markers
            {
                'pattern': r'(?:–í–æ–ø—Ä–æ—Å[—ã–∏]?|Question[s]?)\s*:?\s*(.{10,500}?\?)',
                'confidence': 0.95,
                'type': 'direct_question'
            },
            {
                'pattern': r'(?:–ó–∞–¥–∞–Ω–∏[–µ—è]|Task[s]?|Assignment[s]?)\s*:?\s*(.{10,500}?)(?=\n|$)',
                'confidence': 0.90,
                'type': 'assignment'
            },
            # Numbered questions
            {
                'pattern': r'^\s*\d+[\.\)]\s+(.{10,500}?\?)',
                'confidence': 0.85,
                'type': 'numbered_question'
            },
            # Command-style questions
            {
                'pattern': r'(?:–û—Ç–≤–µ—Ç—å—Ç–µ|–û–±—ä—è—Å–Ω–∏—Ç–µ|–°—Ä–∞–≤–Ω–∏—Ç–µ|–û–ø—Ä–µ–¥–µ–ª–∏—Ç–µ|–ù–∞–∑–æ–≤–∏—Ç–µ|–û—Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏–∑—É–π—Ç–µ|–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ)\s+(.{10,500}?)(?=\n|\.|\?|!)',
                'confidence': 0.80,
                'type': 'command_question'
            },
            # WH-questions
            {
                'pattern': r'(?:–ö–∞–∫|–ß—Ç–æ|–ì–¥–µ|–ö–æ–≥–¥–∞|–ü–æ—á–µ–º—É|–ó–∞—á–µ–º|–ö–∞–∫–æ–π|–ö–∞–∫–∞—è|–ö–∞–∫–∏–µ|–ö—Ç–æ)\s+([^.!?]{10,500}?\?)',
                'confidence': 0.75,
                'type': 'wh_question'
            },
            # Discussion prompts
            {
                'pattern': r'(?:–û–±—Å—É–¥–∏—Ç–µ|–ü–æ–¥—É–º–∞–π—Ç–µ|–†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ)\s+(.{10,500}?)(?=\n|$)',
                'confidence': 0.70,
                'type': 'discussion'
            }
        ]
        
        for pattern_info in question_patterns:
            matches = re.findall(
                pattern_info['pattern'], 
                text, 
                re.MULTILINE | re.IGNORECASE
            )
            
            for match in matches:
                question_text = match.strip()
                
                # Validate question quality
                if self._validate_question_quality(question_text):
                    entities['questions'].append({
                        'text': question_text,
                        'page': page_num,
                        'type': pattern_info['type'],
                        'confidence': pattern_info['confidence'],
                        'length': len(question_text),
                        'complexity_score': self._calculate_question_complexity(question_text)
                    })
    
    def _extract_media_references(self, text, page_num, entities):
        """Extract film and media references"""
        
        media_patterns = [
            {
                'pattern': r'(?:—Ñ–∏–ª—å–º[—ã–∏]?|–∫–∏–Ω–æ|–∫–∏–Ω–æ—Ñ–∏–ª—å–º[—ã–∏]?|–¥–æ–∫—É–º–µ–Ω—Ç–∞–ª—å–Ω[—ã–∏]–π\s+—Ñ–∏–ª—å–º)[:\s]*([^\n]{5,200})',
                'confidence': 0.90,
                'type': 'film'
            },
            {
                'pattern': r'(?:–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º[—ã–∏]?\s+–∫\s+–ø—Ä–æ—Å–º–æ—Ç—Ä—É|–°–º–æ—Ç—Ä–∏—Ç–µ|–ö\s+–ø—Ä–æ—Å–º–æ—Ç—Ä—É)[:\s]*([^\n]{5,200})',
                'confidence': 0.85,
                'type': 'recommendation'
            },
            {
                'pattern': r'(?:–í–∏–¥–µ–æ|–í–∏–¥–µ–æ—Ä–æ–ª–∏–∫|–í–∏–¥–µ–æ—Ñ—Ä–∞–≥–º–µ–Ω—Ç)[:\s]*([^\n]{5,200})',
                'confidence': 0.80,
                'type': 'video'
            }
        ]
        
        for pattern_info in media_patterns:
            matches = re.findall(pattern_info['pattern'], text, re.IGNORECASE)
            for match in matches:
                entities['film_references'].append({
                    'text': match.strip(),
                    'page': page_num,
                    'type': pattern_info['type'],
                    'confidence': pattern_info['confidence']
                })
    
    def _extract_internet_sources(self, text, page_num, entities):
        """Extract internet sources and digital resources"""
        
        url_patterns = [
            {
                'pattern': r'(?:https?://|www\.)[^\s\n]{5,100}',
                'confidence': 0.95,
                'type': 'url'
            },
            {
                'pattern': r'[–∞-—è—ë\w\-]+\.(?:ru|com|org|net|—Ä—Ñ|edu|gov)(?:/[^\s\n]*)?',
                'confidence': 0.85,
                'type': 'domain'
            },
            {
                'pattern': r'(?:–°–∞–π—Ç|–ü–æ—Ä—Ç–∞–ª|–†–µ—Å—É—Ä—Å|–ò–Ω—Ç–µ—Ä–Ω–µ—Ç[- ]?—Ä–µ—Å—É—Ä—Å)[:\s]*([^\n]{5,150})',
                'confidence': 0.80,
                'type': 'web_resource'
            }
        ]
        
        for pattern_info in url_patterns:
            matches = re.findall(pattern_info['pattern'], text, re.IGNORECASE)
            for match in matches:
                entities['internet_sources'].append({
                    'text': match.strip(),
                    'page': page_num,
                    'type': pattern_info['type'],
                    'confidence': pattern_info['confidence']
                })
    
    def _extract_maps_and_diagrams(self, text, page_num, entities):
        """Extract references to maps, diagrams, and visual materials"""
        
        visual_patterns = [
            {
                'pattern': r'(?:–∫–∞—Ä—Ç–∞|—Å—Ö–µ–º–∞|–ø–ª–∞–Ω|–¥–∏–∞–≥—Ä–∞–º–º–∞|–≥—Ä–∞—Ñ–∏–∫|—Ç–∞–±–ª–∏—Ü–∞)[^\n]{0,100}',
                'confidence': 0.85,
                'type': 'visual_reference'
            },
            {
                'pattern': r'(?:—Ä–∏—Å\.|—Ä–∏—Å—É–Ω–æ–∫|–∏–ª–ª—é—Å—Ç—Ä–∞—Ü–∏—è)\s*\d*[^\n]{0,100}',
                'confidence': 0.90,
                'type': 'figure_reference'
            },
            {
                'pattern': r'(?:—Å–º\.|—Å–º–æ—Ç—Ä–∏|—Å–º–æ—Ç—Ä–∏—Ç–µ)\s+(?:–∫–∞—Ä—Ç[—É—ã]|—Å—Ö–µ–º[—É—ã]|—Ä–∏—Å\.|—Ä–∏—Å—É–Ω–æ–∫)[^\n]{0,100}',
                'confidence': 0.95,
                'type': 'visual_instruction'
            }
        ]
        
        for pattern_info in visual_patterns:
            matches = re.findall(pattern_info['pattern'], text, re.IGNORECASE)
            for match in matches:
                entities['maps_and_diagrams'].append({
                    'text': match.strip(),
                    'page': page_num,
                    'type': pattern_info['type'],
                    'confidence': pattern_info['confidence']
                })
    
    def _extract_historical_entities(self, text, page_num, entities):
        """Extract historical figures, dates, and periods"""
        
        # Historical figures (Russian names pattern)
        figure_pattern = r'\b[–ê-–Ø–Å][–∞-—è—ë]+\s+[–ê-–Ø–Å][–∞-—è—ë]+(?:\s+[–ê-–Ø–Å][–∞-—è—ë]+)?\b'
        potential_figures = re.findall(figure_pattern, text)
        
        # Filter out common words and validate as historical figures
        historical_keywords = ['–∫–Ω—è–∑—å', '—Ü–∞—Ä—å', '–∏–º–ø–µ—Ä–∞—Ç–æ—Ä', '–∫–æ—Ä–æ–ª—å', '–≤–µ–ª–∏–∫–∏–π', '—Å–≤—è—Ç–æ–π', '–ø–æ–ª–∫–æ–≤–æ–¥–µ—Ü']
        for figure in potential_figures:
            if any(keyword in text.lower() for keyword in historical_keywords):
                entities['historical_figures'].append({
                    'text': figure,
                    'page': page_num,
                    'confidence': 0.70,
                    'context': self._extract_context(text, figure, 50)
                })
        
        # Dates and periods
        date_patterns = [
            r'\b\d{1,2}\s+(?:—è–Ω–≤–∞—Ä—è|—Ñ–µ–≤—Ä–∞–ª—è|–º–∞—Ä—Ç–∞|–∞–ø—Ä–µ–ª—è|–º–∞—è|–∏—é–Ω—è|–∏—é–ª—è|–∞–≤–≥—É—Å—Ç–∞|—Å–µ–Ω—Ç—è–±—Ä—è|–æ–∫—Ç—è–±—Ä—è|–Ω–æ—è–±—Ä—è|–¥–µ–∫–∞–±—Ä—è)\s+\d{3,4}',
            r'\b\d{3,4}\s*–≥\.?',
            r'\b(?:–≤|—Å|–¥–æ|–ø–æ—Å–ª–µ)\s+\d{3,4}',
            r'\b\d{1,2}-\d{1,2}\s+–≤–µ–∫[–∞–∏]?\b'
        ]
        
        for pattern in date_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            for match in matches:
                entities['dates_and_periods'].append({
                    'text': match,
                    'page': page_num,
                    'confidence': 0.80
                })
    
    def _validate_question_quality(self, question_text):
        """Validate if extracted text is actually a meaningful question"""
        if not question_text or len(question_text) < self.config['entities']['min_question_length']:
            return False
        
        if len(question_text) > self.config['entities']['max_question_length']:
            return False
        
        # Check for question markers
        question_markers = ['?', '–∫–∞–∫', '—á—Ç–æ', '–≥–¥–µ', '–∫–æ–≥–¥–∞', '–ø–æ—á–µ–º—É', '–∑–∞—á–µ–º', '–∫–∞–∫–æ–π', '–∫—Ç–æ']
        has_marker = any(marker in question_text.lower() for marker in question_markers)
        
        # Check for common non-question patterns
        false_positives = ['http', 'www', '@', '—Ç–µ–ª.', '—Ñ–∞–∫—Å']
        has_false_positive = any(fp in question_text.lower() for fp in false_positives)
        
        return has_marker and not has_false_positive
    
    def _calculate_question_complexity(self, question_text):
        """Calculate complexity score for a question"""
        complexity_keywords = [
            '—Å—Ä–∞–≤–Ω–∏—Ç–µ', '–ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ', '–æ—Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏–∑—É–π—Ç–µ', '–æ–±–æ—Å–Ω—É–π—Ç–µ', 
            '–æ–±—ä—è—Å–Ω–∏—Ç–µ', '–¥–æ–∫–∞–∂–∏—Ç–µ', '–æ—Ü–µ–Ω–∏—Ç–µ', '–∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–π—Ç–µ'
        ]
        
        complexity_score = 0
        for keyword in complexity_keywords:
            if keyword in question_text.lower():
                complexity_score += 1
        
        # Normalize to 0-1 range
        return min(complexity_score / len(complexity_keywords), 1.0)
    
    def _extract_context(self, text, entity, context_length=50):
        """Extract context around an entity"""
        start_pos = text.find(entity)
        if start_pos == -1:
            return ""
        
        start = max(0, start_pos - context_length)
        end = min(len(text), start_pos + len(entity) + context_length)
        
        return text[start:end].strip()
    
    def analyze_similarity_with_cache(self, source_entities, suspect_entities):
        """Perform similarity analysis with caching"""
        
        # Create cache key based on both entity sets
        combined_hash = hashlib.md5(
            (str(source_entities) + str(suspect_entities)).encode()
        ).hexdigest()
        
        cache_path = self.cache_dir / "analysis" / f"similarity_{combined_hash}.json"
        cached_analysis = self._load_cache(cache_path)
        
        if cached_analysis:
            return cached_analysis
        
        print(f"üîÑ Performing similarity analysis...")
        
        analysis = {
            'text_similarity': self._analyze_text_similarity(source_entities, suspect_entities),
            'entity_similarity': self._analyze_entity_similarity(source_entities, suspect_entities),
            'structural_comparison': self._compare_structure(source_entities, suspect_entities),
            'borrowing_analysis': self._analyze_borrowings(source_entities, suspect_entities),
            'confidence_scores': {}
        }
        
        # Calculate overall confidence
        analysis['overall_confidence'] = self._calculate_overall_confidence(analysis)
        
        # Save to cache
        self._save_human_readable_cache(
            analysis, cache_path,
            "Similarity Analysis Results"
        )
        
        return analysis
    
    def _analyze_text_similarity(self, source_entities, suspect_entities):
        """Analyze text similarity between source and suspect"""
        print("üìä –ü—Ä–æ–≤–æ–¥–∏–º –¥–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–æ–≤–æ–π —Å—Ö–æ–∂–µ—Å—Ç–∏...")

        # Load raw text data from cache
        source_text_data = self._load_text_from_cache('source')
        suspect_text_data = self._load_text_from_cache('suspect')

        if not source_text_data or not suspect_text_data:
            print("‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ –∫–µ—à–∞")
            return {
                'page_similarities': [],
                'overall_similarity': 0.0,
                'method_comparison': {},
                'suspicious_matches': [],
                'matching_phrases': []
            }

        page_similarities = []
        suspicious_matches = []
        overall_similarities = []
        all_matching_phrases = []

        print(f"üìÑ –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º {len(source_text_data)} —Å—Ç—Ä–∞–Ω–∏—Ü –∏—Å—Ç–æ—á–Ω–∏–∫–∞ —Å {len(suspect_text_data)} —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º–∏ –ø–æ–¥–æ–∑—Ä–µ–≤–∞–µ–º–æ–≥–æ")

        # Compare each source page with each suspect page
        for source_page in source_text_data:
            source_text = source_page.get('text', '').strip()
            if not source_text or len(source_text) < 100:  # Skip very short texts
                continue

            best_match = {'similarity': 0.0, 'suspect_page': None}

            for suspect_page in suspect_text_data:
                suspect_text = suspect_page.get('text', '').strip()
                if not suspect_text or len(suspect_text) < 100:
                    continue

                similarity = self._calculate_text_similarity(source_text, suspect_text)

                if similarity['average'] > best_match['similarity']:
                    best_match = {
                        'similarity': similarity['average'],
                        'suspect_page': suspect_page['page_number'],
                        'source_page': source_page['page_number'],
                        'similarity_details': similarity,
                        'source_text': source_text,
                        'suspect_text': suspect_text
                    }

            if best_match['similarity'] > 0.1:  # Only record meaningful similarities
                page_similarities.append(best_match)
                overall_similarities.append(best_match['similarity'])

                if best_match['similarity'] > self.config['similarity']['threshold']:
                    # Extract matching phrases for this pair
                    phrases = self._extract_matching_phrases(
                        best_match['source_text'],
                        best_match['suspect_text'],
                        best_match['source_page'],
                        best_match['suspect_page']
                    )
                    all_matching_phrases.extend(phrases)

                    suspicious_matches.append({
                        'source_page': best_match['source_page'],
                        'suspect_page': best_match['suspect_page'],
                        'similarity': best_match['similarity_details'],
                        'source_text_preview': source_text[:200] + "..." if len(source_text) > 200 else source_text,
                        'suspect_text_preview': suspect_text[:200] + "..." if len(suspect_text) > 200 else suspect_text,
                        'matching_phrases_count': len(phrases)
                    })

        overall_similarity = sum(overall_similarities) / len(overall_similarities) if overall_similarities else 0.0

        print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(suspicious_matches)} –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π")
        print(f"üìä –û–±—â–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å —Ç–µ–∫—Å—Ç–∞: {overall_similarity:.3f}")
        print(f"üîç –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(all_matching_phrases)} —Å–æ–≤–ø–∞–¥–∞—é—â–∏—Ö —Ñ—Ä–∞–∑")

        return {
            'page_similarities': page_similarities,
            'overall_similarity': overall_similarity,
            'method_comparison': {
                'total_comparisons': len(source_text_data) * len(suspect_text_data),
                'meaningful_similarities': len(page_similarities),
                'suspicious_matches': len(suspicious_matches)
            },
            'suspicious_matches': suspicious_matches,
            'matching_phrases': all_matching_phrases
        }
    
    def _analyze_entity_similarity(self, source_entities, suspect_entities):
        """Analyze similarity in extracted entities"""
        entity_comparison = {}
        
        for entity_type in ['questions', 'film_references', 'internet_sources']:
            source_items = source_entities.get(entity_type, [])
            suspect_items = suspect_entities.get(entity_type, [])
            
            entity_comparison[entity_type] = {
                'source_count': len(source_items),
                'suspect_count': len(suspect_items),
                'matches': self._find_entity_matches(source_items, suspect_items),
                'similarity_percentage': 0.0
            }
        
        return entity_comparison
    
    def _find_entity_matches(self, source_items, suspect_items):
        """Find matching entities between source and suspect"""
        matches = []
        
        for suspect_item in suspect_items:
            best_match = None
            best_similarity = 0.0
            
            for source_item in source_items:
                similarity = self._calculate_text_similarity(
                    suspect_item.get('text', ''),
                    source_item.get('text', '')
                )
                
                if similarity['average'] > best_similarity:
                    best_similarity = similarity['average']
                    best_match = {
                        'source': source_item,
                        'suspect': suspect_item,
                        'similarity': similarity
                    }
            
            if best_similarity > self.config['similarity']['threshold']:
                matches.append(best_match)
        
        return matches
    
    def _load_text_from_cache(self, file_type):
        """Load raw text data from cache files"""
        # Determine which file to load based on file_type
        if file_type == 'source':
            file_path = self.source_path
        elif file_type == 'suspect':
            file_path = self.suspect_path
        else:
            return None

        # Get the cache path for this file
        cache_path = self._get_cache_path("ocr_text", file_path, "_text")

        if not cache_path.exists():
            print(f"‚ö†Ô∏è  –ù–µ –Ω–∞–π–¥–µ–Ω —Ñ–∞–π–ª –∫–µ—à–∞ –¥–ª—è {file_type}")
            return None

        try:
            with open(cache_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                return data.get('data', {}).get('text_pages', [])
        except Exception as e:
            print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–µ—à–∞ –¥–ª—è {file_type}: {e}")
            return None

    def _extract_matching_phrases(self, source_text, suspect_text, source_page, suspect_page):
        """Extract matching phrases between two texts using SequenceMatcher"""
        matches = []
        min_phrase_length = self.config['similarity']['min_phrase_length']
        min_words = self.config['similarity']['min_words']
        paraphrase_threshold = self.config['similarity']['paraphrase_threshold']

        # Normalize texts
        source_normalized = re.sub(r'\s+', ' ', source_text.lower().strip())
        suspect_normalized = re.sub(r'\s+', ' ', suspect_text.lower().strip())

        # 1. Find exact matching blocks using SequenceMatcher
        matcher = SequenceMatcher(None, source_normalized, suspect_normalized)
        matching_blocks = matcher.get_matching_blocks()

        for block in matching_blocks:
            source_start, suspect_start, length = block

            # Only consider matches of significant length
            if length >= min_phrase_length:
                source_phrase = source_normalized[source_start:source_start + length].strip()
                suspect_phrase = suspect_normalized[suspect_start:suspect_start + length].strip()

                # Validate that phrase contains actual words, not just spaces/punctuation
                if len(source_phrase.split()) >= min_words:
                    matches.append({
                        'source_page': source_page,
                        'suspect_page': suspect_page,
                        'source_phrase': source_phrase,
                        'suspect_phrase': suspect_phrase,
                        'length': length,
                        'similarity': 1.0,  # Exact match
                        'type': 'exact'
                    })

        # 2. Find paraphrased sentences using SequenceMatcher
        source_sentences = re.split(r'[.!?]\s+', source_text)
        suspect_sentences = re.split(r'[.!?]\s+', suspect_text)

        for src_sent in source_sentences:
            if len(src_sent) < 30:  # Skip very short sentences
                continue

            src_sent_norm = re.sub(r'\s+', ' ', src_sent.lower().strip())

            for susp_sent in suspect_sentences:
                if len(susp_sent) < 30:
                    continue

                susp_sent_norm = re.sub(r'\s+', ' ', susp_sent.lower().strip())

                # Calculate sentence similarity using SequenceMatcher
                sent_similarity = SequenceMatcher(None, src_sent_norm, susp_sent_norm).ratio()

                # If sentences are similar but not exact matches (paraphrases)
                if paraphrase_threshold <= sent_similarity < 1.0:
                    matches.append({
                        'source_page': source_page,
                        'suspect_page': suspect_page,
                        'source_phrase': src_sent.strip(),
                        'suspect_phrase': susp_sent.strip(),
                        'length': max(len(src_sent), len(susp_sent)),
                        'similarity': sent_similarity,
                        'type': 'paraphrase'
                    })

        return matches

    def _calculate_text_similarity(self, text1, text2):
        """Calculate similarity between two texts using SequenceMatcher only"""
        if not text1 or not text2:
            return {'average': 0.0, 'sequence_matcher': 0.0}

        # Clean text
        text1_clean = re.sub(r'\s+', ' ', text1.lower().strip())
        text2_clean = re.sub(r'\s+', ' ', text2.lower().strip())

        # Calculate similarity using SequenceMatcher
        seq_similarity = SequenceMatcher(None, text1_clean, text2_clean).ratio()

        return {
            'sequence_matcher': seq_similarity,
            'average': seq_similarity  # Average is just sequence_matcher now
        }
    
    def _compare_structure(self, source_entities, suspect_entities):
        """Compare structural elements"""
        return {
            'page_count_ratio': 1.0,  # Placeholder
            'content_density_comparison': {},
            'entity_distribution': {}
        }
    
    def _analyze_borrowings(self, source_entities, suspect_entities):
        """Analyze potential borrowings"""
        return {
            'text_borrowing_percentage': 0.0,
            'entity_borrowing_percentage': 0.0,
            'high_confidence_matches': [],
            'suspicious_patterns': []
        }
    
    def _calculate_overall_confidence(self, analysis):
        """Calculate overall confidence in analysis results"""
        return 0.85  # Placeholder
    
    def run_full_pipeline(self):
        """Run the complete analysis pipeline"""
        print("üöÄ Starting Comprehensive Textbook Analysis Pipeline")
        print("=" * 70)
        
        # Step 1: Extract text with caching
        print("\nüìñ Step 1: Text Extraction")
        source_text = self.extract_text_with_cache(self.source_path)
        suspect_text = self.extract_text_with_cache(self.suspect_path)
        
        # Step 2: Extract entities with caching
        print("\nüîç Step 2: Entity Extraction")  
        source_entities = self.extract_entities_with_cache(source_text)
        suspect_entities = self.extract_entities_with_cache(suspect_text)
        
        # Step 3: Similarity analysis with caching
        print("\n‚öñÔ∏è Step 3: Similarity Analysis")
        similarity_analysis = self.analyze_similarity_with_cache(source_entities, suspect_entities)
        
        # Step 4: Generate comprehensive report
        print("\nüìä Step 4: Report Generation")
        self.results = {
            'source_text': source_text,
            'suspect_text': suspect_text,
            'source_entities': source_entities,
            'suspect_entities': suspect_entities,
            'similarity_analysis': similarity_analysis,
            'pipeline_metadata': {
                'analysis_date': datetime.now().isoformat(),
                'source_file': self.source_path,
                'suspect_file': self.suspect_path,
                'cache_directory': str(self.cache_dir),
                'configuration': self.config
            }
        }
        
        return self.results
    
    def generate_comprehensive_report(self, output_dir="client"):
        """Generate human-readable comprehensive report"""
        if not self.results:
            print("‚ùå No analysis results found. Run pipeline first.")
            return

        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)

        # Generate base filename from source and suspect files
        source_name = Path(self.source_path).stem
        suspect_name = Path(self.suspect_path).stem
        base_name = f"{source_name}_vs_{suspect_name}"

        # Create dedicated folder for this pair
        pair_folder = output_path / base_name
        pair_folder.mkdir(exist_ok=True)

        # 1. Save main report
        report_path = pair_folder / "analysis_report.md"
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(self._generate_markdown_report())

        # 2. Save matching phrases
        phrases_path = pair_folder / "matching_phrases.txt"
        with open(phrases_path, 'w', encoding='utf-8') as f:
            f.write(self._generate_matching_phrases_report())

        # 3. Save normalized texts
        source_text_path = pair_folder / "source_normalized_text.txt"
        suspect_text_path = pair_folder / "suspect_normalized_text.txt"

        with open(source_text_path, 'w', encoding='utf-8') as f:
            f.write(self._generate_normalized_text(self.results['source_text']))

        with open(suspect_text_path, 'w', encoding='utf-8') as f:
            f.write(self._generate_normalized_text(self.results['suspect_text']))

        # 4. Save detailed JSON data
        data_path = pair_folder / "detailed_data.json"
        with open(data_path, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2, default=str)

        print(f"üìã Reports generated in {pair_folder}/:")
        print(f"  ‚Ä¢ analysis_report.md")
        print(f"  ‚Ä¢ matching_phrases.txt")
        print(f"  ‚Ä¢ source_normalized_text.txt")
        print(f"  ‚Ä¢ suspect_normalized_text.txt")
        print(f"  ‚Ä¢ detailed_data.json")

    def _generate_method_explanation(self):
        """Generate method explanation for non-mathematical client"""
        return r"""# –û–ë–™–Ø–°–ù–ï–ù–ò–ï –ú–ï–¢–û–î–ê –ê–ù–ê–õ–ò–ó–ê –ù–ê –ü–õ–ê–ì–ò–ê–¢

## –î–ª—è –∫–æ–≥–æ —ç—Ç–æ—Ç –¥–æ–∫—É–º–µ–Ω—Ç
–≠—Ç–æ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–æ –¥–ª—è –ª—é–¥–µ–π –±–µ–∑ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è. –ú—ã –æ–±—ä—è—Å–Ω–∏–º, –∫–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞—à –∞–Ω–∞–ª–∏–∑ –ø—Ä–æ—Å—Ç—ã–º —è–∑—ã–∫–æ–º.

## –ß—Ç–æ –¥–µ–ª–∞–µ—Ç –Ω–∞—à–∞ —Å–∏—Å—Ç–µ–º–∞

### 1. –ò–ó–í–õ–ï–ß–ï–ù–ò–ï –¢–ï–ö–°–¢–ê –ò–ó PDF
**–ß—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç:** –°–∏—Å—Ç–µ–º–∞ —á–∏—Ç–∞–µ—Ç PDF-—Ñ–∞–π–ª—ã —É—á–µ–±–Ω–∏–∫–æ–≤ –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç –∏–∑ –Ω–∏—Ö —Ç–µ–∫—Å—Ç.

**–ö–∞–∫ —ç—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç:**
- –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Ç–µ–∫—Å—Ç –Ω–∞–ø—Ä—è–º—É—é (–µ—Å–ª–∏ PDF —Å–æ–¥–µ—Ä–∂–∏—Ç –Ω–∞—Å—Ç–æ—è—â–∏–π —Ç–µ–∫—Å—Ç)
- –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç –Ω–µ —á–∏—Ç–∞–µ—Ç—Å—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, —É—á–µ–±–Ω–∏–∫ –æ—Ç—Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω –∫–∞–∫ –∫–∞—Ä—Ç–∏–Ω–∫–∞), –∏—Å–ø–æ–ª—å–∑—É–µ–º OCR (–æ–ø—Ç–∏—á–µ—Å–∫–æ–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Å–∏–º–≤–æ–ª–æ–≤)
- OCR ‚Äî —ç—Ç–æ –∫–∞–∫ –∫–æ–≥–¥–∞ –∫–æ–º–ø—å—é—Ç–µ—Ä "—Å–º–æ—Ç—Ä–∏—Ç" –Ω–∞ –∫–∞—Ä—Ç–∏–Ω–∫—É –∏ —Ä–∞—Å–ø–æ–∑–Ω–∞—ë—Ç –Ω–∞ –Ω–µ–π –±—É–∫–≤—ã

**–ß—Ç–æ –ø–æ–ª—É—á–∞–µ–º:**
- –¢–µ–∫—Å—Ç –∫–∞–∂–¥–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
- –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–æ–º, —Å–∫–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –∫–∞–∂–¥–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ
- –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç–∏ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è (–¥–ª—è OCR)

### 2. –ü–û–ò–°–ö –û–ë–†–ê–ó–û–í–ê–¢–ï–õ–¨–ù–´–• –≠–õ–ï–ú–ï–ù–¢–û–í
**–ß—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç:** –°–∏—Å—Ç–µ–º–∞ –∏—â–µ—Ç –≤ —Ç–µ–∫—Å—Ç–µ –≤–∞–∂–Ω—ã–µ –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã.

**–ß—Ç–æ –∏—â–µ–º:**
- **–í–æ–ø—Ä–æ—Å—ã –∏ –∑–∞–¥–∞–Ω–∏—è** ‚Äî –ø–æ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–º —Å–ª–æ–≤–∞–º-–º–∞—Ä–∫–µ—Ä–∞–º: "–í–æ–ø—Ä–æ—Å:", "–ó–∞–¥–∞–Ω–∏–µ:", "–û–±—ä—è—Å–Ω–∏—Ç–µ", "–°—Ä–∞–≤–Ω–∏—Ç–µ" –∏ —Ç.–¥.
- **–°—Å—ã–ª–∫–∏ –Ω–∞ —Ñ–∏–ª—å–º—ã** ‚Äî —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö —Ñ–∏–ª—å–º–æ–≤, —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –∫ –ø—Ä–æ—Å–º–æ—Ç—Ä—É
- **–ò–Ω—Ç–µ—Ä–Ω–µ—Ç-—Ä–µ—Å—É—Ä—Å—ã** ‚Äî –≤–µ–±-—Å–∞–π—Ç—ã, –ø–æ—Ä—Ç–∞–ª—ã, –æ–Ω–ª–∞–π–Ω-–º–∞—Ç–µ—Ä–∏–∞–ª—ã
- **–ö–∞—Ä—Ç—ã –∏ —Å—Ö–µ–º—ã** ‚Äî —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤
- **–ò—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –ø–µ—Ä—Å–æ–Ω—ã –∏ –¥–∞—Ç—ã** ‚Äî –∏–º–µ–Ω–∞, –¥–∞—Ç—ã, –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ø–µ—Ä–∏–æ–¥—ã

**–ó–∞—á–µ–º —ç—Ç–æ –Ω—É–∂–Ω–æ:**
–≠—Ç–∏ —ç–ª–µ–º–µ–Ω—Ç—ã –ø–æ–º–æ–≥–∞—é—Ç –ø–æ–Ω—è—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—É —É—á–µ–±–Ω–∏–∫–∞. –ï—Å–ª–∏ –≤–æ–ø—Ä–æ—Å—ã –æ—á–µ–Ω—å –ø–æ—Ö–æ–∂–∏ ‚Äî —ç—Ç–æ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—Ä–∏–∑–Ω–∞–∫ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è.

### 3. –°–†–ê–í–ù–ï–ù–ò–ï –¢–ï–ö–°–¢–û–í
**–ß—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç:** –°–∏—Å—Ç–µ–º–∞ —Å—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç—ã –¥–≤—É—Ö —É—á–µ–±–Ω–∏–∫–æ–≤, —á—Ç–æ–±—ã –Ω–∞–π—Ç–∏ –ø–æ—Ö–æ–∂–∏–µ –º–µ—Å—Ç–∞.

**–ö–∞–∫ –º—ã —Å—Ä–∞–≤–Ω–∏–≤–∞–µ–º:**

#### –ú–µ—Ç–æ–¥ 1: –ü–æ—Å–∏–º–≤–æ–ª—å–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ (SequenceMatcher)
- –ü—Ä–µ–¥—Å—Ç–∞–≤—å—Ç–µ, —á—Ç–æ –º—ã –∫–ª–∞–¥—ë–º –¥–≤–∞ —Ç–µ–∫—Å—Ç–∞ —Ä—è–¥–æ–º –∏ —Å–º–æ—Ç—Ä–∏–º, —Å–∫–æ–ª—å–∫–æ –±—É–∫–≤ —Å–æ–≤–ø–∞–¥–∞—é—Ç –ø–æ–¥—Ä—è–¥
- –ß–µ–º –±–æ–ª—å—à–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π ‚Äî —Ç–µ–º –≤—ã—à–µ —Å—Ö–æ–∂–µ—Å—Ç—å
- –ü—Ä–∏–º–µ—Ä: "–ò—Å—Ç–æ—Ä–∏—è –†–æ—Å—Å–∏–∏" –∏ "–ò—Å—Ç–æ—Ä–∏—è –£–∫—Ä–∞–∏–Ω—ã" —Å–æ–≤–ø–∞–¥–∞—é—Ç –Ω–∞ 50% (–ø–µ—Ä–≤–∞—è —á–∞—Å—Ç—å –æ–¥–∏–Ω–∞–∫–æ–≤–∞—è)

**–ò—Ç–æ–≥–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞:**
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –ø–æ—Å–∏–º–≤–æ–ª—å–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —á–µ—Ä–µ–∑ SequenceMatcher ‚Äî —ç—Ç–æ —Å–∞–º—ã–π –ø—Ä–æ—Å—Ç–æ–π –∏ –ø–æ–Ω—è—Ç–Ω—ã–π –º–µ—Ç–æ–¥, –∫–æ—Ç–æ—Ä—ã–π –Ω–µ —Ç—Ä–µ–±—É–µ—Ç —Å–ª–æ–∂–Ω—ã—Ö –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π.

### 4. –ò–ó–í–õ–ï–ß–ï–ù–ò–ï –û–î–ò–ù–ê–ö–û–í–´–• –§–†–ê–ó
**–ß—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç:** –°–∏—Å—Ç–µ–º–∞ –Ω–∞—Ö–æ–¥–∏—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –∫—É—Å–∫–∏ —Ç–µ–∫—Å—Ç–∞, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–≤—Ç–æ—Ä—è—é—Ç—Å—è –≤ –æ–±–æ–∏—Ö —É—á–µ–±–Ω–∏–∫–∞—Ö.

**–ö–∞–∫ —ç—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç:**

#### –¢–æ—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è:
- –ò—â–µ–º –±–ª–æ–∫–∏ —Ç–µ–∫—Å—Ç–∞ –¥–ª–∏–Ω–æ–π –º–∏–Ω–∏–º—É–º 50 —Å–∏–º–≤–æ–ª–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–ª–Ω–æ—Å—Ç—å—é —Å–æ–≤–ø–∞–¥–∞—é—Ç
- –î–æ–ª–∂–Ω–æ –±—ã—Ç—å –º–∏–Ω–∏–º—É–º 5 —Å–ª–æ–≤ –ø–æ–¥—Ä—è–¥
- –ü—Ä–∏–º–µ—Ä: "–ë–∏—Ç–≤–∞ –Ω–∞ –ö—É–ª–∏–∫–æ–≤–æ–º –ø–æ–ª–µ –ø—Ä–æ–∏–∑–æ—à–ª–∞ –≤ 1380 –≥–æ–¥—É –∏ —Å—Ç–∞–ª–∞ –ø–µ—Ä–µ–ª–æ–º–Ω—ã–º –º–æ–º–µ–Ω—Ç–æ–º"

#### –ü–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è:
- –ò—â–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ—Ö–æ–∂–∏ –Ω–∞ 70% –∏ –±–æ–ª–µ–µ, –Ω–æ –Ω–µ –∏–¥–µ–Ω—Ç–∏—á–Ω—ã
- –≠—Ç–æ –ø–æ–º–æ–≥–∞–µ—Ç –Ω–∞–π—Ç–∏ —Ç–µ–∫—Å—Ç, –∫–æ—Ç–æ—Ä—ã–π –ø–µ—Ä–µ–ø–∏—Å–∞–ª–∏ –¥—Ä—É–≥–∏–º–∏ —Å–ª–æ–≤–∞–º–∏
- –ü—Ä–∏–º–µ—Ä:
  - –û—Ä–∏–≥–∏–Ω–∞–ª: "–ü—ë—Ç—Ä I –ø—Ä–æ–≤—ë–ª –º–∞—Å—à—Ç–∞–±–Ω—ã–µ —Ä–µ—Ñ–æ—Ä–º—ã –∞—Ä–º–∏–∏ –∏ —Ñ–ª–æ—Ç–∞"
  - –ü–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∫–∞: "–ü–µ—Ç—Ä–æ–º –ü–µ—Ä–≤—ã–º –±—ã–ª–∏ –æ—Å—É—â–µ—Å—Ç–≤–ª–µ–Ω—ã –∫—Ä—É–ø–Ω—ã–µ –≤–æ–µ–Ω–Ω—ã–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è"

### 5. –ö–†–ò–¢–ï–†–ò–ò –ü–û–î–û–ó–†–ò–¢–ï–õ–¨–ù–û–°–¢–ò
**–ö–æ–≥–¥–∞ –º—ã —Å—á–∏—Ç–∞–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–º:**
- –û–±—â–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü > 15% (–Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º—ã–π –ø–æ—Ä–æ–≥)
- –ù–∞–π–¥–µ–Ω—ã –±–ª–æ–∫–∏ —Å–æ–≤–ø–∞–¥–∞—é—â–µ–≥–æ —Ç–µ–∫—Å—Ç–∞ –¥–ª–∏–Ω–æ–π 50+ —Å–∏–º–≤–æ–ª–æ–≤
- –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –ø–æ—Ö–æ–∂–∏–µ (70%+) –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è

## –ß—Ç–æ –ù–ï —Å—á–∏—Ç–∞–µ—Ç—Å—è –ø–ª–∞–≥–∏–∞—Ç–æ–º
- –û–¥–∏–Ω–∞–∫–æ–≤—ã–µ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ —Ñ–∞–∫—Ç—ã (–¥–∞—Ç—ã, —Å–æ–±—ã—Ç–∏—è ‚Äî –æ–Ω–∏ –æ–±—ä–µ–∫—Ç–∏–≤–Ω—ã)
- –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ —É—á–µ–±–Ω—ã–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏
- –û–±—â–µ—É–ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª—å–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã
- –ö–æ—Ä–æ—Ç–∫–∏–µ —Ñ—Ä–∞–∑—ã (–º–µ–Ω—å—à–µ 5 —Å–ª–æ–≤)

## –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –º–µ—Ç–æ–¥–∞
1. **OCR –º–æ–∂–µ—Ç –æ—à–∏–±–∞—Ç—å—Å—è** ‚Äî –µ—Å–ª–∏ —É—á–µ–±–Ω–∏–∫ –ø–ª–æ—Ö–æ –æ—Ç—Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω, —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ—Ç–æ—á–Ω—ã–º
2. **–ù–µ —É—á–∏—Ç—ã–≤–∞–µ—Ç—Å—è —Å–º—ã—Å–ª –Ω–∞ 100%** ‚Äî —Å–∏—Å—Ç–µ–º–∞ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—É—é –ø–æ—Ö–æ–∂–µ—Å—Ç—å, –∞ –Ω–µ –≥–ª—É–±–æ–∫–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è
3. **–ù—É–∂–µ–Ω –∫–æ–Ω—Ç–µ–∫—Å—Ç** ‚Äî –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ —Ñ—Ä–∞–∑—ã –º–æ–≥—É—Ç –±—ã—Ç—å —Å–ª—É—á–∞–π–Ω—ã–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ–º –∏–ª–∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–π —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–æ–π

## –ö–∞–∫ —á–∏—Ç–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
- **–ü—Ä–æ—Ü–µ–Ω—Ç —Å—Ö–æ–∂–µ—Å—Ç–∏ 0-10%** ‚Äî –Ω–æ—Ä–º–∞–ª—å–Ω–æ, —É—á–µ–±–Ω–∏–∫–∏ –ø—Ä–æ –æ–¥–Ω—É —Ç–µ–º—É
- **–ü—Ä–æ—Ü–µ–Ω—Ç —Å—Ö–æ–∂–µ—Å—Ç–∏ 10-30%** ‚Äî —Ç—Ä–µ–±—É–µ—Ç –≤–Ω–∏–º–∞–Ω–∏—è, –≤–æ–∑–º–æ–∂–Ω—ã –∑–∞–∏–º—Å—Ç–≤–æ–≤–∞–Ω–∏—è
- **–ü—Ä–æ—Ü–µ–Ω—Ç —Å—Ö–æ–∂–µ—Å—Ç–∏ 30%+** ‚Äî –≤—ã—Å–æ–∫–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è
- **–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–≤–ø–∞–¥–∞—é—â–∏—Ö —Ñ—Ä–∞–∑** ‚Äî —Å–º–æ—Ç—Ä–∏—Ç–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã, –æ–Ω–∏ –≤–∞–∂–Ω–µ–µ –æ–±—â–µ–≥–æ –ø—Ä–æ—Ü–µ–Ω—Ç–∞

## –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ (—É—Å–∫–æ—Ä–µ–Ω–∏–µ –ø–æ–≤—Ç–æ—Ä–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞)
–°–∏—Å—Ç–µ–º–∞ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã:
- –ò–∑–≤–ª–µ—á—ë–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
- –ù–∞–π–¥–µ–Ω–Ω—ã–µ –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
- –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å—Ä–∞–≤–Ω–µ–Ω–∏—è

–ü—Ä–∏ –ø–æ–≤—Ç–æ—Ä–Ω–æ–º –∑–∞–ø—É—Å–∫–µ –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ—Ö –∂–µ —Ñ–∞–π–ª–æ–≤ —Å–∏—Å—Ç–µ–º–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ ‚Äî —ç—Ç–æ –≤ –¥–µ—Å—è—Ç–∫–∏ —Ä–∞–∑ –±—ã—Å—Ç—Ä–µ–µ.

---
*–≠—Ç–æ—Ç –º–µ—Ç–æ–¥ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º –Ω–∞—É—á–Ω—ã–º –ø–æ–¥—Ö–æ–¥–∞–º –∫ –¥–µ—Ç–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—é —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∑–∞–∏–º—Å—Ç–≤–æ–≤–∞–Ω–∏–π*
"""

    def _generate_matching_phrases_report(self):
        """Generate report with all matching phrases"""
        if 'similarity_analysis' not in self.results:
            return "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –æ —Å–æ–≤–ø–∞–¥–∞—é—â–∏—Ö —Ñ—Ä–∞–∑–∞—Ö"

        matching_phrases = self.results['similarity_analysis']['text_similarity'].get('matching_phrases', [])

        if not matching_phrases:
            return "–°–æ–≤–ø–∞–¥–∞—é—â–∏–µ —Ñ—Ä–∞–∑—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã"

        report = f"""# –°–û–í–ü–ê–î–ê–Æ–©–ò–ï –§–†–ê–ó–´
# –ù–∞–π–¥–µ–Ω–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π: {len(matching_phrases)}

"""

        # Group by page pairs
        page_pairs = {}
        for phrase in matching_phrases:
            key = (phrase['source_page'], phrase['suspect_page'])
            if key not in page_pairs:
                page_pairs[key] = []
            page_pairs[key].append(phrase)

        for (src_page, susp_page), phrases in sorted(page_pairs.items()):
            report += f"\n## –°—Ç—Ä–∞–Ω–∏—Ü–∞ {src_page} (–∏—Å—Ç–æ—á–Ω–∏–∫) ‚Üî –°—Ç—Ä–∞–Ω–∏—Ü–∞ {susp_page} (–ø–æ–¥–æ–∑—Ä–µ–≤–∞–µ–º—ã–π)\n"
            report += f"–ù–∞–π–¥–µ–Ω–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π: {len(phrases)}\n\n"

            for i, phrase in enumerate(phrases, 1):
                phrase_type = phrase.get('type', 'exact')
                similarity = phrase.get('similarity', 1.0)

                if phrase_type == 'exact':
                    report += f"### –°–æ–≤–ø–∞–¥–µ–Ω–∏–µ #{i} (—Ç–æ—á–Ω–æ–µ, {phrase['length']} —Å–∏–º–≤–æ–ª–æ–≤)\n"
                else:
                    report += f"### –°–æ–≤–ø–∞–¥–µ–Ω–∏–µ #{i} (–ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∫–∞, —Å—Ö–æ–∂–µ—Å—Ç—å {similarity:.1%})\n"

                report += f"**–ò—Å—Ç–æ—á–Ω–∏–∫:** {phrase['source_phrase']}\n\n"
                report += f"**–ü–æ–¥–æ–∑—Ä–µ–≤–∞–µ–º—ã–π:** {phrase['suspect_phrase']}\n\n"
                report += "---\n\n"

        return report

    def _generate_normalized_text(self, text_data):
        """Generate normalized text from extraction data"""
        output = []
        output.append(f"# –ò–ó–í–õ–ï–ß–Å–ù–ù–´–ô –ò –ù–û–†–ú–ê–õ–ò–ó–û–í–ê–ù–ù–´–ô –¢–ï–ö–°–¢")
        output.append(f"# –§–∞–π–ª: {text_data['source_file']}")
        output.append(f"# –í—Å–µ–≥–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {text_data['total_pages']}")
        output.append(f"# –°—Ç—Ä–∞–Ω–∏—Ü —Å —Ç–µ–∫—Å—Ç–æ–º: {text_data['pages_with_text']}")
        output.append(f"# –ú–µ—Ç–æ–¥ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è: {', '.join(text_data['extraction_method'])}")
        output.append(f"# –í—Å–µ–≥–æ —Å–∏–º–≤–æ–ª–æ–≤: {text_data['extraction_stats']['total_characters']:,}")
        output.append("\n" + "="*80 + "\n")

        for page_data in text_data.get('text_pages', []):
            output.append(f"\n--- –°–¢–†–ê–ù–ò–¶–ê {page_data['page_number']} ---")
            output.append(f"–ú–µ—Ç–æ–¥: {page_data['method']}, –°–∏–º–≤–æ–ª–æ–≤: {page_data['char_count']}, –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {page_data.get('confidence', 1.0):.2f}")
            output.append("")
            output.append(page_data['text'])
            output.append("\n" + "-"*80 + "\n")

        return "\n".join(output)
    
    def _generate_markdown_report(self):
        """Generate markdown format report"""
        source_text = self.results['source_text']
        suspect_text = self.results['suspect_text']
        source_entities = self.results['source_entities']
        suspect_entities = self.results['suspect_entities']
        similarity_analysis = self.results.get('similarity_analysis', {})

        # Extract similarity metrics
        text_similarity = similarity_analysis.get('text_similarity', {})
        overall_similarity = text_similarity.get('overall_similarity', 0.0)
        matching_phrases = text_similarity.get('matching_phrases', [])
        suspicious_matches = text_similarity.get('suspicious_matches', [])

        # Calculate exact and paraphrase matches
        exact_matches = [p for p in matching_phrases if p.get('type') == 'exact']
        paraphrase_matches = [p for p in matching_phrases if p.get('type') == 'paraphrase']

        report = f"""# TEXTBOOK PLAGIARISM ANALYSIS REPORT

**Analysis Date:** {datetime.now().strftime('%d %B %Y, %H:%M')}
**Source Textbook:** {os.path.basename(self.source_path)}
**Suspect Textbook:** {os.path.basename(self.suspect_path)}

## SIMILARITY ANALYSIS

### Overall Text Similarity: {overall_similarity:.1%}

| Metric | Value |
|--------|-------|
| Overall Similarity Score | {overall_similarity:.1%} |
| Suspicious Page Matches | {len(suspicious_matches)} |
| Total Matching Phrases | {len(matching_phrases)} |
| Exact Matches | {len(exact_matches)} |
| Paraphrase Matches | {len(paraphrase_matches)} |

### Interpretation
- **0-10%**: Normal overlap for textbooks on similar topics
- **10-30%**: Requires attention, possible borrowing
- **30%+**: High probability of plagiarism

## TEXT EXTRACTION STATISTICS

| Metric | Source | Suspect | Ratio |
|--------|---------|---------|--------|
| Total Pages | {source_text['total_pages']} | {suspect_text['total_pages']} | {suspect_text['total_pages']/source_text['total_pages']:.2f} |
| Pages with Text | {source_text['pages_with_text']} | {suspect_text['pages_with_text']} | {suspect_text['pages_with_text']/source_text['pages_with_text']:.2f} |
| Total Characters | {source_text['extraction_stats']['total_characters']:,} | {suspect_text['extraction_stats']['total_characters']:,} | {suspect_text['extraction_stats']['total_characters']/source_text['extraction_stats']['total_characters']:.2f} |
| Images | {len(source_text['images'])} | {len(suspect_text['images'])} | {len(suspect_text['images'])/max(len(source_text['images']),1):.2f} |

## ENTITY EXTRACTION STATISTICS

| Entity Type | Source Count | Suspect Count | Ratio |
|-------------|--------------|---------------|--------|
| Questions | {len(source_entities['questions'])} | {len(suspect_entities['questions'])} | {len(suspect_entities['questions'])/max(len(source_entities['questions']),1):.2f} |
| Film References | {len(source_entities['film_references'])} | {len(suspect_entities['film_references'])} | {len(suspect_entities['film_references'])/max(len(source_entities['film_references']),1):.2f} |
| Internet Sources | {len(source_entities['internet_sources'])} | {len(suspect_entities['internet_sources'])} | {len(suspect_entities['internet_sources'])/max(len(source_entities['internet_sources']),1):.2f} |
| Maps/Diagrams | {len(source_entities['maps_and_diagrams'])} | {len(suspect_entities['maps_and_diagrams'])} | {len(suspect_entities['maps_and_diagrams'])/max(len(source_entities['maps_and_diagrams']),1):.2f} |

---
*Analysis generated: {datetime.now().strftime('%d %B %Y, %H:%M')}*
"""
        return report

def main():
    """Main execution function"""
    print("üéØ COMPREHENSIVE TEXTBOOK ANALYSIS PIPELINE")
    print("=" * 60)

    # Define book pairs to analyze
    book_pairs = [
        ("books/6 –∫–ª–∞—Å—Å. –ò—Å—Ç–æ—Ä–∏—è –†–æ—Å—Å–∏–∏.pdf", "books/6 –∫–ª–∞—Å—Å –≤—Ç–æ—Ä–∞—è –≥—Ä—É–ø–ø–∞.pdf"),
        ("books/7 –∫–ª–∞—Å—Å. –ò—Å—Ç–æ—Ä–∏—è –†–æ—Å—Å–∏–∏.pdf", "books/7 –∫–ª–∞—Å—Å –≤—Ç–æ—Ä–∞—è –≥—Ä—É–ø–ø–∞.pdf"),
        ("books/8 –∫–ª–∞—Å—Å. –ò—Å—Ç–æ—Ä–∏—è –†–æ—Å—Å–∏–∏.pdf", "books/8 –∫–ª–∞—Å—Å –≤—Ç–æ—Ä–∞—è –≥—Ä—É–ø–ø–∞.pdf")
    ]

    output_dir = "client"

    for i, (source_pdf, suspect_pdf) in enumerate(book_pairs, 1):
        print(f"\n{'='*70}")
        print(f"üìö –ê–ù–ê–õ–ò–ó –ü–ê–†–´ {i}/{len(book_pairs)}")
        print(f"{'='*70}")
        print(f"–ò—Å—Ç–æ—á–Ω–∏–∫: {os.path.basename(source_pdf)}")
        print(f"–ü–æ–¥–æ–∑—Ä–µ–≤–∞–µ–º—ã–π: {os.path.basename(suspect_pdf)}")

        if not os.path.exists(source_pdf) or not os.path.exists(suspect_pdf):
            print(f"‚ùå PDF —Ñ–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã!")
            continue

        # Initialize analyzer
        analyzer = CachedTextbookAnalyzer(source_pdf, suspect_pdf)

        try:
            # Run pipeline
            results = analyzer.run_full_pipeline()

            # Generate reports
            analyzer.generate_comprehensive_report(output_dir=output_dir)

            print(f"\n‚úÖ –ê–Ω–∞–ª–∏–∑ –ø–∞—Ä—ã {i} –∑–∞–≤–µ—Ä—à—ë–Ω!")

        except Exception as e:
            print(f"‚ùå –ê–Ω–∞–ª–∏–∑ –Ω–µ —É–¥–∞–ª—Å—è: {e}")
            import traceback
            traceback.print_exc()

    print(f"\n{'='*70}")
    print(f"üéâ –í–°–ï –ê–ù–ê–õ–ò–ó–´ –ó–ê–í–ï–†–®–ï–ù–´!")
    print(f"üìÅ –í—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {output_dir}/")
    print(f"{'='*70}")

if __name__ == "__main__":
    main()
